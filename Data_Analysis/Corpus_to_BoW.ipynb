{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b09346c",
   "metadata": {},
   "source": [
    "# 1.1 Load a corpus using file method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d18f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi how are you. I am fine', 'I am studying Converastional AI', 'I love to explore technology', 'Converastional AI is my favourite subject', 'This semester I have five subjects', 'My laptop have display issues', 'This subject is amazing', 'We can intergrate AI with any technology', 'Survival of the fittest', 'I am studying in Thapar University']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "filenames = os.listdir('E:/Thapar/SEM5/DataAnalysis/Lab/Dataset2')\n",
    "content = []\n",
    "for file in filenames:\n",
    "    f = open('E:/Thapar/SEM5/DataAnalysis/Lab/Dataset2/'+file, 'r')\n",
    "    text = f.read()\n",
    "    content.append(text)\n",
    "    f.close()\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d500bf83",
   "metadata": {},
   "source": [
    "# 1.2 Load a corpus using PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a30828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a363d6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=PlaintextCorpusReader('E:/Thapar/SEM5/DataAnalysis/Lab/Dataset2/', '.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ac20ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a.txt',\n",
       " 'b.txt',\n",
       " 'c.txt',\n",
       " 'd.txt',\n",
       " 'e.txt',\n",
       " 'f.txt',\n",
       " 'g.txt',\n",
       " 'h.txt',\n",
       " 'i.txt',\n",
       " 'j.txt']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7e37a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi how are you. I am fine', 'I am studying Converastional AI', 'I love to explore technology', 'Converastional AI is my favourite subject', 'This semester I have five subjects', 'My laptop have display issues', 'This subject is amazing', 'We can intergrate AI with any technology', 'Survival of the fittest', 'I am studying in Thapar University']\n"
     ]
    }
   ],
   "source": [
    "content1 = []\n",
    "for file in data.fileids():\n",
    "    content1.append(''.join(data.raw(fileids=file)))\n",
    "print(content1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c67280a",
   "metadata": {},
   "source": [
    "# 2.1 Preprocessing: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd0bb477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi how are i am fine', 'i am studying converastional ai', 'i love to explore technology', 'converastional ai is my favourite subject', 'this semester i have five subjects', 'my laptop have display issues', 'this subject is amazing', 'we can intergrate ai with any technology', 'survival of the fittest', 'i am studying in thapar university']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "normalize = []\n",
    "for line in content:\n",
    "    normalize.append(' '.join([word.lower() for word in line.split() if word.isalpha()]))\n",
    "print(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b00ad8",
   "metadata": {},
   "source": [
    "# 2.2 Preprocessing: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f11be7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi', 'how', 'are', 'i', 'am', 'fine'], ['i', 'am', 'studying', 'converastional', 'ai'], ['i', 'love', 'to', 'explore', 'technology'], ['converastional', 'ai', 'is', 'my', 'favourite', 'subject'], ['this', 'semester', 'i', 'have', 'five', 'subjects'], ['my', 'laptop', 'have', 'display', 'issues'], ['this', 'subject', 'is', 'amazing'], ['we', 'can', 'intergrate', 'ai', 'with', 'any', 'technology'], ['survival', 'of', 'the', 'fittest'], ['i', 'am', 'studying', 'in', 'thapar', 'university']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenize = []\n",
    "for line in normalize:\n",
    "    tokenize.append(nltk.word_tokenize(line))\n",
    "print(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9425633",
   "metadata": {},
   "source": [
    "# 2.3 Preprocessing: Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc26ac0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import *\n",
    "stopword_lst = nltk.corpus.stopwords.words(fileids='english')\n",
    "print(stopword_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "643a4b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hi', 'fine'], ['studying', 'converastional', 'ai'], ['love', 'explore', 'technology'], ['converastional', 'ai', 'favourite', 'subject'], ['semester', 'five', 'subjects'], ['laptop', 'display', 'issues'], ['subject', 'amazing'], ['intergrate', 'ai', 'technology'], ['survival', 'fittest'], ['studying', 'thapar', 'university']]\n"
     ]
    }
   ],
   "source": [
    "nostop = []\n",
    "for line in tokenize:\n",
    "    nostop.append([word for word in line if word not in stopword_lst])\n",
    "print(nostop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2121c139",
   "metadata": {},
   "source": [
    "# 2.4 Preprocessing: Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c219dd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi fine', 'studi converast ai', 'love explor technolog', 'converast ai favourit subject', 'semest five subject', 'laptop display issu', 'subject amaz', 'intergr ai technolog', 'surviv fittest', 'studi thapar univers']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stem_data = []\n",
    "for line in nostop:\n",
    "    stem_data.append(' '.join([ps.stem(word) for word in line]))\n",
    "print(stem_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb7fdd",
   "metadata": {},
   "source": [
    "# 3.A.1 Corpus into Bag-of-Words using CountVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a7769b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(stem_data)\n",
    "#print(X.toarray())\n",
    "#print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1de3bb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>amaz</th>\n",
       "      <th>converast</th>\n",
       "      <th>display</th>\n",
       "      <th>explor</th>\n",
       "      <th>favourit</th>\n",
       "      <th>fine</th>\n",
       "      <th>fittest</th>\n",
       "      <th>five</th>\n",
       "      <th>hi</th>\n",
       "      <th>...</th>\n",
       "      <th>issu</th>\n",
       "      <th>laptop</th>\n",
       "      <th>love</th>\n",
       "      <th>semest</th>\n",
       "      <th>studi</th>\n",
       "      <th>subject</th>\n",
       "      <th>surviv</th>\n",
       "      <th>technolog</th>\n",
       "      <th>thapar</th>\n",
       "      <th>univers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b.txt</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d.txt</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h.txt</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j.txt</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       ai  amaz  converast  display  explor  favourit  fine  fittest  five  \\\n",
       "a.txt   0     0          0        0       0         0     1        0     0   \n",
       "b.txt   1     0          1        0       0         0     0        0     0   \n",
       "c.txt   0     0          0        0       1         0     0        0     0   \n",
       "d.txt   1     0          1        0       0         1     0        0     0   \n",
       "e.txt   0     0          0        0       0         0     0        0     1   \n",
       "f.txt   0     0          0        1       0         0     0        0     0   \n",
       "g.txt   0     1          0        0       0         0     0        0     0   \n",
       "h.txt   1     0          0        0       0         0     0        0     0   \n",
       "i.txt   0     0          0        0       0         0     0        1     0   \n",
       "j.txt   0     0          0        0       0         0     0        0     0   \n",
       "\n",
       "       hi  ...  issu  laptop  love  semest  studi  subject  surviv  technolog  \\\n",
       "a.txt   1  ...     0       0     0       0      0        0       0          0   \n",
       "b.txt   0  ...     0       0     0       0      1        0       0          0   \n",
       "c.txt   0  ...     0       0     1       0      0        0       0          1   \n",
       "d.txt   0  ...     0       0     0       0      0        1       0          0   \n",
       "e.txt   0  ...     0       0     0       1      0        1       0          0   \n",
       "f.txt   0  ...     1       1     0       0      0        0       0          0   \n",
       "g.txt   0  ...     0       0     0       0      0        1       0          0   \n",
       "h.txt   0  ...     0       0     0       0      0        0       0          1   \n",
       "i.txt   0  ...     0       0     0       0      0        0       1          0   \n",
       "j.txt   0  ...     0       0     0       0      1        0       0          0   \n",
       "\n",
       "       thapar  univers  \n",
       "a.txt       0        0  \n",
       "b.txt       0        0  \n",
       "c.txt       0        0  \n",
       "d.txt       0        0  \n",
       "e.txt       0        0  \n",
       "f.txt       0        0  \n",
       "g.txt       0        0  \n",
       "h.txt       0        0  \n",
       "i.txt       0        0  \n",
       "j.txt       1        1  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bow = pd.DataFrame(data=X.toarray(), columns=cv.get_feature_names(), index=filenames)\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a8b42f",
   "metadata": {},
   "source": [
    "# 3.A.2 Corpus into Bag-of-Words using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e9ef6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(norm=False, smooth_idf=False)\n",
    "X1=tf.fit_transform(stem_data)\n",
    "#print(X1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf69ddaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ai</th>\n",
       "      <th>amaz</th>\n",
       "      <th>converast</th>\n",
       "      <th>display</th>\n",
       "      <th>explor</th>\n",
       "      <th>favourit</th>\n",
       "      <th>fine</th>\n",
       "      <th>fittest</th>\n",
       "      <th>five</th>\n",
       "      <th>hi</th>\n",
       "      <th>...</th>\n",
       "      <th>issu</th>\n",
       "      <th>laptop</th>\n",
       "      <th>love</th>\n",
       "      <th>semest</th>\n",
       "      <th>studi</th>\n",
       "      <th>subject</th>\n",
       "      <th>surviv</th>\n",
       "      <th>technolog</th>\n",
       "      <th>thapar</th>\n",
       "      <th>univers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b.txt</th>\n",
       "      <td>2.203973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d.txt</th>\n",
       "      <td>2.203973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.203973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.203973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.203973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h.txt</th>\n",
       "      <td>2.203973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j.txt</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.609438</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.302585</td>\n",
       "      <td>3.302585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ai      amaz  converast   display    explor  favourit      fine  \\\n",
       "a.txt  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  3.302585   \n",
       "b.txt  2.203973  0.000000   2.609438  0.000000  0.000000  0.000000  0.000000   \n",
       "c.txt  0.000000  0.000000   0.000000  0.000000  3.302585  0.000000  0.000000   \n",
       "d.txt  2.203973  0.000000   2.609438  0.000000  0.000000  3.302585  0.000000   \n",
       "e.txt  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "f.txt  0.000000  0.000000   0.000000  3.302585  0.000000  0.000000  0.000000   \n",
       "g.txt  0.000000  3.302585   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "h.txt  2.203973  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "i.txt  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "j.txt  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        fittest      five        hi  ...      issu    laptop      love  \\\n",
       "a.txt  0.000000  0.000000  3.302585  ...  0.000000  0.000000  0.000000   \n",
       "b.txt  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "c.txt  0.000000  0.000000  0.000000  ...  0.000000  0.000000  3.302585   \n",
       "d.txt  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "e.txt  0.000000  3.302585  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "f.txt  0.000000  0.000000  0.000000  ...  3.302585  3.302585  0.000000   \n",
       "g.txt  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "h.txt  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "i.txt  3.302585  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "j.txt  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "\n",
       "         semest     studi   subject    surviv  technolog    thapar   univers  \n",
       "a.txt  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "b.txt  0.000000  2.609438  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "c.txt  0.000000  0.000000  0.000000  0.000000   2.609438  0.000000  0.000000  \n",
       "d.txt  0.000000  0.000000  2.203973  0.000000   0.000000  0.000000  0.000000  \n",
       "e.txt  3.302585  0.000000  2.203973  0.000000   0.000000  0.000000  0.000000  \n",
       "f.txt  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.000000  \n",
       "g.txt  0.000000  0.000000  2.203973  0.000000   0.000000  0.000000  0.000000  \n",
       "h.txt  0.000000  0.000000  0.000000  0.000000   2.609438  0.000000  0.000000  \n",
       "i.txt  0.000000  0.000000  0.000000  3.302585   0.000000  0.000000  0.000000  \n",
       "j.txt  0.000000  2.609438  0.000000  0.000000   0.000000  3.302585  3.302585  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "bow1 = pd.DataFrame(data=X1.toarray(), columns=tf.get_feature_names(), index=filenames)\n",
    "bow1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b618fa",
   "metadata": {},
   "source": [
    "# 3.B Corpus into Bag-of-Words without using in-built functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c8f47ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_count = {}\n",
    "for i in range(len(stem_data)):\n",
    "    word_count = {}\n",
    "    for word in stem_data[i].split():\n",
    "        if word not in word_count.keys():\n",
    "          word_count[word] = 0\n",
    "        word_count[word] += 1\n",
    "    doc_count[filenames[i]] = word_count\n",
    "bow2 = pd.DataFrame(doc_count)\n",
    "#bow2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e4c905c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a.txt</th>\n",
       "      <th>b.txt</th>\n",
       "      <th>c.txt</th>\n",
       "      <th>d.txt</th>\n",
       "      <th>e.txt</th>\n",
       "      <th>f.txt</th>\n",
       "      <th>g.txt</th>\n",
       "      <th>h.txt</th>\n",
       "      <th>i.txt</th>\n",
       "      <th>j.txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>hi</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fine</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studi</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>converast</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>explor</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technolog</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>favourit</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subject</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>semest</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>five</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>laptop</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>display</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issu</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amaz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intergr</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>surviv</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fittest</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thapar</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>univers</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           a.txt  b.txt  c.txt  d.txt  e.txt  f.txt  g.txt  h.txt  i.txt  \\\n",
       "hi           1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "fine         1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "studi        0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "converast    0.0    1.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "ai           0.0    1.0    0.0    1.0    0.0    0.0    0.0    1.0    0.0   \n",
       "love         0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "explor       0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "technolog    0.0    0.0    1.0    0.0    0.0    0.0    0.0    1.0    0.0   \n",
       "favourit     0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0    0.0   \n",
       "subject      0.0    0.0    0.0    1.0    1.0    0.0    1.0    0.0    0.0   \n",
       "semest       0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "five         0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0   \n",
       "laptop       0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0   \n",
       "display      0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0   \n",
       "issu         0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0    0.0   \n",
       "amaz         0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0    0.0   \n",
       "intergr      0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0    0.0   \n",
       "surviv       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "fittest      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    1.0   \n",
       "thapar       0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "univers      0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "           j.txt  \n",
       "hi           0.0  \n",
       "fine         0.0  \n",
       "studi        1.0  \n",
       "converast    0.0  \n",
       "ai           0.0  \n",
       "love         0.0  \n",
       "explor       0.0  \n",
       "technolog    0.0  \n",
       "favourit     0.0  \n",
       "subject      0.0  \n",
       "semest       0.0  \n",
       "five         0.0  \n",
       "laptop       0.0  \n",
       "display      0.0  \n",
       "issu         0.0  \n",
       "amaz         0.0  \n",
       "intergr      0.0  \n",
       "surviv       0.0  \n",
       "fittest      0.0  \n",
       "thapar       1.0  \n",
       "univers      1.0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow2.fillna(0,inplace=True)\n",
    "bow2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
